"""
Author: Bernard
Description:

    This script creates a .csv file with all the scores computed
    for different dimensionality reduction algorithms and various
    hyper-parameter configurations.

    .. note: It uses GridSearch and thus the models that do not
             have the method transform fail when computing the
             scores using 'custom_metrics'.

"""

# Libraries
import yaml
import pandas as pd
import numpy as np

from time import time
from pathlib import Path
from datetime import datetime
from sklearn.pipeline import Pipeline
from sklearn.model_selection import ParameterGrid
from sklearn.model_selection import GridSearchCV
from utils.settings import _FEATURES
from utils.settings import _LABELS
from utils.settings import _IMPUTERS
from utils.settings import _SCALERS
from utils.settings import _METHODS

# ----------------------------------
# Methods
# ----------------------------------

def custom_metrics(est, X, y):
    """This method computes the metrics.

    .. todo: Check whether the X value received here has
             already gone through previous steps such as
             inputation of missing data or preprocessing.
             The X value is is raw data in fit.

    .. todo: Check the outcomes of encoder, encode_inputs,
             transform, ... so that they are all consistent.

    .. note: The X and y represent the values passed to the
             .fit(X, y) method below. In general, y is either
             the class (classification) or a value (regression).

    .. note: Since we received the estimator, we need to apply
             the prediction/transformation ourselves manually
             as shown below.

    .. note: The scoring function must return numbers, thus
             including any string value such as the uuid or
             an autogenerated uuid4 does not work here.

    Parameters
    ----------
    est: object
        The estimator or pipeline.
    X:  np.array (or dataframe)
        The X data in fit.
    y: np.array (or dataframe)
        The y data in fit.
    """
    # Transform
    y_embd = est.transform(X)
    # Metrics
    m = custom_metrics_(X, y_embd, y)
    # Additional

    # Return
    return m


def custom_metrics_(y_true, y_pred, y, n=1000):
    """This method computes the metrics.

    .. note: computation of pairwise distances takes too
             long when datasets are big. Thus, instead
             select a random number of examples and compute
             on them.

    Parameters
    ----------
    y_true: np.array (dataframe)
        Array with original data (X).
    y_pred: np.array
        Array with transformed data (y_emb).
    y: np.array (dataframe)
        Array with the outcomes
    n: int
        The number of samples to use for distance metrics.

    Returns
    -------
    dict-like
        Dictionary with the scores.
    """
    # Libraries
    from scipy.spatial.distance import cdist
    from scipy.spatial import procrustes
    from scipy.stats import spearmanr, pearsonr
    from sklearn.metrics import silhouette_score
    from sklearn.metrics import calinski_harabasz_score
    from sklearn.metrics import davies_bouldin_score
    from ls2d.metrics import gmm_scores
    from ls2d.metrics import gmm_ratio_score
    from ls2d.metrics import gmm_intersection_matrix
    from ls2d.metrics import gmm_intersection_area

    # Reduce computations which are expensive
    N = n if len(y_true) > n else len(y_true)
    idx = np.random.choice(np.arange(len(y_true)), N, replace=False)
    y_true_ = y_true.iloc[idx, :]
    y_pred_ = y_pred[idx]

    # Compute distances
    true_dist = cdist(y_true_, y_true_).flatten()
    pred_dist = cdist(y_pred_, y_pred_).flatten()

    # Compute scores
    pearson = pearsonr(true_dist, pred_dist)
    spearman = spearmanr(true_dist, pred_dist)

    # Compute procrustes
    try:
        mtx1, mtx2, disparity = procrustes(y_true, y_pred)
    except ValueError as e:
        mtx1, mtx2, disparity = None, None, -1

    # Compute scores for selected outcomes
    #d_gmm = {}
    for c in y.columns:
        try:
            idx = y[c].notna()
            y_true_, y_pred_, y_ = \
                y_true[idx], y_pred[idx], y[c][idx]
            #d_ = gmm_scores(y_pred_, y_)
            d_ = {}
            d_['silhouette'] = silhouette_score(y_pred_, y_)
            d_['calinski'] = calinski_harabasz_score(y_pred_, y_.ravel())
            d_['davies_b'] = davies_bouldin_score(y_pred_, y_.ravel())
            d_ = {'%s_%s'%(k, c) : v for k,v in d_.items()}
            #d_gmm.update(d_)
        except Exception as e:
            print("Error: %s" % e)

    # Create dictioanry
    d = {}
    d['pearson'] = pearson[0]
    d['spearman'] = spearman[0]
    d['procrustes'] = disparity
    #d.update(d_gmm)

    # Return
    return d


# ----------------------------------
# Load arguments
# ----------------------------------
# Library
import argparse

# Default example (iris)
DEFAULT_YAML = './03-ls2d-loop.yaml'

# Load
parser = argparse.ArgumentParser()
parser.add_argument("--yaml", type=str, nargs='?',
                    const=DEFAULT_YAML, default=DEFAULT_YAML,
                    help="yaml configuration file")
args = parser.parse_args()



# ----------------------------------
# Set configuration
# ----------------------------------
# Library
from utils.utils import AttrDict

# Load configuration from file
with open(Path(args.yaml)) as file:
    CONFIG = AttrDict(yaml.full_load(file))

# Configuration
grid = [
    {
        'imputer': ['simp'],
        'scaler': ['mmx', 'std'],
        'method': ['pca', 'icaf', 'iso']#'tsne', 'mds', 'spe', 'umap']
    },
    {
        'imputer': ['simp'],
        'scaler': ['mmx'],
        'method': ['lda', 'nmf']
    }
]

"""
grid = [
    {
        'imputer': ['simp'],
        'scaler': ['mmx'],
        'method': ['pca']
    }
]
"""

grid = ParameterGrid(grid)




# ----------------------------------
# Load data
# ----------------------------------
# Load data from csv file.
data = pd.read_csv(Path(CONFIG.datapath))

# .. note: This lines is ensuring that only those observations
#          which are complete (all features available) are used
#          for training the models.
data = data.dropna(how='any', subset=CONFIG.features)

# Show data
print("\nData:")
print(data)
print("\nDtypes:")
print(data.dtypes)
print("\nOrganisms:")
print(data.micro_code.value_counts())

# Create X and y
X = data[CONFIG.features]
y = data[CONFIG.targets]


# -------------------------
# Loop
# -------------------------
# Compendium of results
compendium = pd.DataFrame()

# For each estimator
for i, e in enumerate(grid):

    # Create steps
    imputer = e.get('imputer')
    scaler = e.get('scaler')
    method = e.get('method')

    # Create variables
    folder = "%s-%s-%s" % (method, imputer, scaler)

    # Logging
    print("\n%s/%s. Computing... %s" % (i+1, len(grid), folder))

    # Get the param grid
    param_grid = CONFIG.params.get(method, {})
    param_grid = {'method__%s' % k:v for k,v in param_grid.items()}

    # Create pipeline
    pipe = Pipeline([
        ('imputer', _IMPUTERS.get(imputer)),
        ('scaler', _SCALERS.get(scaler)),
        ('method', _METHODS.get(method))
    ])

    # Create grid search (cv is one single fold)
    grid_search = GridSearchCV(pipe, param_grid=param_grid,
                        cv=(((slice(None), slice(None)),)),
                        scoring=custom_metrics,
                        return_train_score=True, verbose=2,
                        refit=False, n_jobs=1)

    # Fit grid search
    grid_search.fit(X, y)

    # Save results as csv
    results = grid_search.cv_results_

    # Add information
    df = pd.DataFrame(results)
    df.insert(1, 'estimator', _METHODS.get(method).__class__.__name__)
    df.insert(0, 'folder', folder)

    # Append to total results
    compendium = compendium.append(df)



# Show
print("Results:")
print(compendium)

# Save grid search rsults
timestamp = datetime.now().strftime('%y%m%d-%H%M%S')
filename = 'gridsearch-%s.csv' % timestamp
compendium.to_csv(Path(CONFIG.outpath) /  filename  , index=False)